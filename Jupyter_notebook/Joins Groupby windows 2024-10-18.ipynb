{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa332f1b-2f52-487f-b4e3-f672dd52c831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=5376419441612485#setting/sparkui/1018-134725-wnliq33b/driver-1446801091498375292\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=5376419441612485#setting/sparkui/1018-134725-wnliq33b/driver-1446801091498375292\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a17a6b2-cf8e-4445-b1ca-49783b8e73f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004504a4-d08f-46eb-8e3f-a3b0bbf4d457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac11029-2497-4655-b9f4-71dc374dcd85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6545945d-b96b-42c9-9f41-17a51c8985cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd3196c-5f08-4e3b-960b-d9dd70e504bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e972d5dd-595b-4475-8489-c2ac20c6cbe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='full').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1581d2-e4c5-442a-8c34-c2573f48b304",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## leftsemi and leftanti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06bbf14-aa34-48cb-86df-f4861ef853e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ddb704-cdb9-4d6c-9b48-4fbf188172fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|     6|Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF['emp_dept_id']==deptDF['dept_id'],how='leftanti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b15c12d-4d0f-45d9-8ed6-10abfb772afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313f6db7-da34-41d6-9eff-7482566999b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## self join to get superior name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e394c37-460a-4b64-aab0-41a13978ee4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n|emp_id|    name|superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|     2|    Rose|              1|            Smith|\n|     3|Williams|              1|            Smith|\n|     4|   Jones|              2|             Rose|\n|     5|   Brown|              2|             Rose|\n|     6|   Brown|              2|             Rose|\n+------+--------+---------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.alias('emp1').join(empDF.alias('emp2'),col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\").\\\n",
    "    select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d06692ba-2f50-4ab3-89bb-dc811e599547",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7876002-e219-443c-adf8-8b56c69bf513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark=spark.read.format('csv').option('header',True).option('inferSchema',True).load('dbfs:/FileStore/employee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e23d0d5-c53e-4806-9928-e1a1f5ddbf7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,max,min,count,avg,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd47d90-d8ba-4203-8fd6-6b0d2a6889c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+---------------+-----------+------+\n|Empid|    Name|            Skills|Date of Joining|       Dept|Salary|\n+-----+--------+------------------+---------------+-----------+------+\n|    1|Abhishek|  python,sql,excel|     2020-01-01|DataScience|100000|\n|    2|  Nitesh|    sap,word,excel|     2017-06-10|    Finance|150000|\n|    3|  Aniket|          Java,sql|     2021-02-20|Development|200000|\n|    4| Sandeep|Aws,postgre,python|     2020-01-01|Development|125000|\n|    5|   Rahul|       Azure,scala|     2018-05-05|DataScience|175000|\n|    6|  Venkat|        Slang,java|     2019-05-10|    Finance|250000|\n|    7| Arunesh|    finacle,isense|     2021-02-20|    Finance|150000|\n|    8|Sangeeta|      Dermatoloist|     2022-10-30|DataScience|100000|\n|    9|  Yuvraj|              ABCD|     2023-06-01|Development|200000|\n|   10|  Tushar|           running|     2021-10-20|    Finance|150000|\n+-----+--------+------------------+---------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fca3b13-6c9f-49e0-a09f-d29cc7a4e4e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark=df_spark.withColumnRenamed('Date of Joining','DOJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b412cb-3156-421e-a570-5f1fe69c7e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|\n+-----+--------+------------------+----------+-----------+------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|\n+-----+--------+------------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62bbaaf-a358-490e-86e6-94078d207e66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|       Dept|sum(Salary)|\n+-----------+-----------+\n|    Finance|     700000|\n|Development|     525000|\n|DataScience|     375000|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('Dept').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372db0b1-58c5-433f-9b17-d6ee2b326f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1036526035208601>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDept\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSalary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: _api() takes 1 positional argument but 2 were given"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1036526035208601>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDept\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSalary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: _api() takes 1 positional argument but 2 were given",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: _api() takes 1 positional argument but 2 were given",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark.groupby(['Dept','name']).count('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0727a6e3-0611-4504-9497-d51a929df1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+-------+\n|       Dept|Total_Salary|Count_emp|Min_sal|\n+-----------+------------+---------+-------+\n|    Finance|      700000|        4| 150000|\n|Development|      525000|        3| 125000|\n|DataScience|      375000|        3| 100000|\n+-----------+------------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby(['Dept']).agg(sum('Salary').alias('Total_Salary'),count('Salary').alias('Count_emp'),min('Salary').alias('Min_sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faab9a56-d328-4cbc-96a4-769b24458e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646e1727-2443-4a3a-98a8-4aaf4d38a78c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|count(DISTINCT Dept)|\n+--------------------+\n|                   3|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(count_distinct(col('Dept'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caa1b44-c031-4169-97b3-f6ff5f18a207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|\n+-----+--------+------------------+----------+-----------+------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|\n+-----+--------+------------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "480af149-7fcf-414d-8337-aa27f1e373f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Windows Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53023eea-82bc-46c6-896b-0f1a99835052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7d6675-ed03-46ef-9bc8-f7cab9fb89ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank,dense_rank,row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e94c1209-8811-492b-b678-62bfe2abf09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+---------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|Dept_wise|\n+-----+--------+------------------+----------+-----------+------+---------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|        1|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|        1|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|        3|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|        1|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|        2|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|        2|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|        1|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|        1|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|        1|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|        4|\n+-----+--------+------------------+----------+-----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('Dept_wise',rank().over(Window.partitionBy(col('Dept')).orderBy(col('Salary')))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc93bf54-5a1a-4649-a0a0-b47eb27d9dfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+--------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|row_rank|\n+-----+--------+------------------+----------+-----------+------+--------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|       1|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|       2|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|       3|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|       1|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|       2|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|       3|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|       1|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|       2|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|       3|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|       4|\n+-----+--------+------------------+----------+-----------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('row_rank',row_number().over(Window.partitionBy('Dept').orderBy('Salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82de43a-47ee-496b-8f4e-278c6dde368a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+----------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|Dense_rank|\n+-----+--------+------------------+----------+-----------+------+----------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|         1|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|         1|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|         2|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|         1|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|         2|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|         2|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|         1|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|         1|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|         1|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|         2|\n+-----+--------+------------------+----------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('Dense_rank',dense_rank().over(Window.partitionBy('Dept').orderBy('Salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c32e58e-d88e-4e66-871d-96587750d50e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+---------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|Total_sum|\n+-----+--------+------------------+----------+-----------+------+---------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|   375000|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|   375000|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|   375000|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|   525000|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|   525000|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|   525000|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|   700000|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|   700000|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|   700000|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|   700000|\n+-----+--------+------------------+----------+-----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('Total_sum',sum('Salary').over(Window.partitionBy('Dept'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f46352e-98e8-4e35-9386-1a5624221eda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+-----+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|count|\n+-----+--------+------------------+----------+-----------+------+-----+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|    3|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|    3|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|    3|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|    3|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|    3|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|    3|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|    4|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|    4|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|    4|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|    4|\n+-----+--------+------------------+----------+-----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('count',count('Salary').over(Window.partitionBy('Dept'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e972d349-a23b-460e-950a-bd01879e52ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+-------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|Max_amt|\n+-----+--------+------------------+----------+-----------+------+-------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000| 175000|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000| 175000|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000| 175000|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000| 200000|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000| 200000|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000| 200000|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000| 250000|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000| 250000|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000| 250000|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000| 250000|\n+-----+--------+------------------+----------+-----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('Max_amt',max('Salary').over(Window.partitionBy('Dept'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9241d3-21c6-441c-917c-7ce05514aa7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ec754a-1751-488b-a0eb-e044e40678ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+-------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|Percent|\n+-----+--------+------------------+----------+-----------+------+-------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|    0.0|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|    0.0|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|    1.0|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|    0.0|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|    0.5|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|    0.5|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|    0.0|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|    0.0|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|    0.0|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|    1.0|\n+-----+--------+------------------+----------+-----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('Percent',percent_rank().over(Window.partitionBy('Dept').orderBy('Salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219340f0-c477-4c62-afff-06ca68a99ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,nth_value,lag,lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4ff1f8-54e4-4bf4-8db9-55ad82d16e0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|ntiles|\n+-----+--------+------------------+----------+-----------+------+------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|     1|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|     2|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|     3|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|     1|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|     2|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|     3|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|     1|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|     1|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|     2|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|     3|\n+-----+--------+------------------+----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('ntiles',ntile(3).over(Window.partitionBy('Dept').orderBy('Salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6723f13d-ffe6-4850-b907-1db7d275337e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+----------+-----------+------+----------+\n|Empid|    Name|            Skills|       DOJ|       Dept|Salary|nth_values|\n+-----+--------+------------------+----------+-----------+------+----------+\n|    1|Abhishek|  python,sql,excel|2020-01-01|DataScience|100000|      null|\n|    8|Sangeeta|      Dermatoloist|2022-10-30|DataScience|100000|      null|\n|    5|   Rahul|       Azure,scala|2018-05-05|DataScience|175000|    175000|\n|    4| Sandeep|Aws,postgre,python|2020-01-01|Development|125000|      null|\n|    3|  Aniket|          Java,sql|2021-02-20|Development|200000|    200000|\n|    9|  Yuvraj|              ABCD|2023-06-01|Development|200000|    200000|\n|    2|  Nitesh|    sap,word,excel|2017-06-10|    Finance|150000|    150000|\n|    7| Arunesh|    finacle,isense|2021-02-20|    Finance|150000|    150000|\n|   10|  Tushar|           running|2021-10-20|    Finance|150000|    150000|\n|    6|  Venkat|        Slang,java|2019-05-10|    Finance|250000|    150000|\n+-----+--------+------------------+----------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumn('nth_values',nth_value('Salary',3).over(Window.partitionBy('Dept').orderBy('Salary'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0afad40-9924-4fc8-8774-b6d17144ae7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Some Other Exapmle from\n",
    "https://sparkbyexamples.com/pyspark/pyspark-window-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d5ec48-f538-483e-bbbf-54f7dd0860f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc59742e-5f8c-4615-b48f-d1dc6d0e4da9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efcf6a8-d959-4674-9e40-b11d6861c48e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a319c7-7153-4a1c-b7bf-ee37fa3e3416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47711ba-407a-4707-80d0-758e8f50ffeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|        Maria|   Finance|  3000|         0.0|\n|        Scott|   Finance|  3300|         0.5|\n|          Jen|   Finance|  3900|         1.0|\n|        Kumar| Marketing|  2000|         0.0|\n|         Jeff| Marketing|  3000|         1.0|\n|        James|     Sales|  3000|         0.0|\n|        James|     Sales|  3000|         0.0|\n|       Robert|     Sales|  4100|         0.5|\n|         Saif|     Sales|  4100|         0.5|\n|      Michael|     Sales|  4600|         1.0|\n+-------------+----------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24890691-6151-41e7-94ae-c40c79263bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|        Maria|   Finance|  3000|    1|\n|        Scott|   Finance|  3300|    1|\n|          Jen|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|         Jeff| Marketing|  3000|    2|\n|        James|     Sales|  3000|    1|\n|        James|     Sales|  3000|    1|\n|       Robert|     Sales|  4100|    1|\n|         Saif|     Sales|  4100|    2|\n|      Michael|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de2f4ce-3f7f-4af3-9579-9440a3cf5629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|        Maria|   Finance|  3000|0.3333333333333333|\n|        Scott|   Finance|  3300|0.6666666666666666|\n|          Jen|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|         Jeff| Marketing|  3000|               1.0|\n|        James|     Sales|  3000|               0.4|\n|        James|     Sales|  3000|               0.4|\n|       Robert|     Sales|  4100|               0.8|\n|         Saif|     Sales|  4100|               0.8|\n|      Michael|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a476cbe6-667d-4ad2-bac5-5088eb1f8534",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|null|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|3000|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ab4f1c-6692-4fbe-a33d-ca5f2b7f558c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12314985-8df6-416d-9faf-374bd5647f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac955cc-b152-4536-8039-4fc094a30fef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Joins Groupby windows 2024-10-18",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
